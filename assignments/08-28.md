# Week of August 28

This assignment is due EOD on September 4, 2025.
Requirements: Task 0 AND Task 1 AND Task 2. (Task 3 is optional.)

## Submission

As you are working, you can [commit and push](https://docs.github.com/en/get-started/using-git/about-git) to your fork. 
To submit the assignment, you will [create a Pull Request](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork) (PR) with the main [collaborative](https://github.com/ml-feedback-sys/collaborative-f25) repository.
Please title the PR "08-28 Submission - `netID`" and include a description of the work that you did.

## Task 0: Create a personal fork of the collaborative repository and a submission python script

First, [fork](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks) the [collaborative](https://github.com/ml-feedback-sys/collaborative-f25) repository.
Then, create a folder in `submission/` named `netID`.

## Task 1: Create and evaluate a linear predictor for video recommendation

Copy the [template script](code/08-28watch_prediction.py) from this repository into your folder.
The template file includes code for loading [KauiRec](https://kuairec.com/) data, training a linear predictor, and evaluating its performance.
The approach is very basic: it uses only a few features from the available data to predict the target. (The target for each user-item pair is the "watch ratio" - see the documentation for more details.)
See if you can modify the setup or learning algorithm to find a better performing predictor.
Some suggestions: use a richer feature representation, transform or normalize the features or labels, tune the regularization hyperparameters, etc.
Comment your code to explain your additions and report your final performance in your PR.


## Task 2: Derive linear least-squares predictor for a linear-Gaussian model

Copy the [template notebook](code/08-28linear-ls.py) from this repository into your folder.
Suppose data follows distribution $\mathcal D$ described as:
$$x\sim \mathcal N(\mu, \Sigma),\quad v\sim \mathcal N(0, \sigma^2),\quad y=\theta^\top x + v$$

Derive an expression for the linear least-squares predictor:
$$\theta_\star = \arg\min_{\theta\in\mathbb R^d} \mathbb E_{x,y\sim\mathcal D}[(\theta^\top x-y)^2] $$
You answer should be in terms of $\mu$, $\Sigma$, $\sigma^2$, and $\theta$.
Also derive an expression for the optimal risk $\mathcal R(\theta_\star)$.

Report the expressions that you derive in your pull request. 
How does the optimal risk depend on  $\mu$, $\Sigma$, $\sigma^2$, and $\theta$?

## (optional) Task 3: Learning from finite samples

Implement a data generation function that samples from the linear-Gaussian model. 
Sample $n$ data points and compute the empirical risk minimizer $\hat\theta$.
Compute both the risk of $\hat\theta$ on 1) the training sample, 2) a new sample of size $n$, and 3) on the whole population (using the theoretical expressions from above).
Compare these to the risk of $\theta_\star$ in all three cases.
What happens as you vary $n$?

