# Week of October 16

This assignment is due EOD on October 23, 2024.
Requirements: Task 1 AND Task 2 AND (Task 3 OR Task 4).

## Submission

As you are working, you can [commit and push](https://docs.github.com/en/get-started/using-git/about-git) to your fork. 
To submit the assignment, you will [create a Pull Request](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork) (PR) with the main [collaborative](https://github.com/ml-feedback-sys/collaborative-f25) repository.
Please title the PR "10-16 Submission - `netID`" and include a description of the work that you did.

Make sure you are keeping your fork [up to date](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/syncing-a-fork).
Any files that you submit should be in your folder in the collaborative repository (`submission/netID`).

## Task 1: One-step optimal control

Consider the following: 
1. At each timestep, you observe the state of a dynamical system which is reset according to a distribution $s_{0,t} \sim \mathcal D_0\in\Delta(\mathbb R^{d_s})$.
2. You select an action $a_t\in\mathbb R^{d_a}$ and pay a cost $s_1^\top Qs_1+a_t^\top R a_t$ where $s_1=Fs_{0,t}+Ga_t$, $Q\succeq 0,R\succ 0$ are cost matrices, and $F,G$ are dynamics matrices.

What is the optimal action in terms of $F,G,Q,R,s_{0,t}$?

## Task 2: Formulation as linear contextual bandits

We now view the one step control problem as an instance of linear contextual bandits. 
We define the context as the reset state $x_t=s_{0,t}$ so that $\mathcal X = \mathbb R^{d_s}$, the action space as $\mathbb R^{d_a}$, and the reward as the negative of the cost.
In order to formulate the problem in terms of a [linear reward model](https://slides.com/sarahdean-2/11-bandits-ml-in-feedback-sys-f25#/6), define
- the feature function $\varphi$ which maps contexts and actions to features
- the linear parameter $\theta_\star$ governing the rewards (should be in terms of $F,G,Q,R$)

What is the optimal action in terms of $\theta_\star$ and $x_t$?

## (pick one) Task 3: Implement and run bandit algorithm

Implement this one-step control setting with dynamics and cost matrices of your choice. 
Create an interface for an algorithm to observe the context, select and action, and then observe a noisy realization of reward.
Treating the parameters $\theta_\star$ as unknown, implement the following simple bandit algorithm:
1. Estimate $\hat\theta_t$ from $\\{x_k,a_k,r_k\\}_{k=0}^t$ using linear regression
2. Compute $\bar a_t$, the optimal action for $\hat\theta_t$ and $x_t$ (using the previously derived expression)
3. Play $a_t = \bar a_t + \epsilon_t$ where $\epsilon_t\sim\mathcal N(0,\sigma_t^2)$.

This algorithm is known as epsilon-greedy.

Try a few different exploration decay rates by varying $\sigma_t$ (e.g. $t^{-1/2}$, $t^{-2/3}$, etc).
Plot both the regret and the parameter estimation error and include these plots in your PR.

## (pick one) Task 4: LQR for multi-step control

Now consider the $T$ step optimal control problem, i.e. LQR.
Implement a function for computing the optimal policy $(K_0,\dots,K_{T})$ in terms of $F,G,Q,R$ for arbitrary $T$.
Also implement a function for computing the optimal steady state policy $K$ (when $T\to\infty$) using `scipy.linalg.solve_discrete_are`. 

Implement a linear system with $d_s=2$ and $d_a=1$ with dynamics of your choice, including a stochastic disturbance, so $s_{t+1} = Fs_t + Ga_t + w_t$. 
Define some cost matrices $Q,R$.
We will investigate the behavior for a handful of horizons $T$, including the steady-state.
For each $T$, compute the LQR policy, and simulate the closed-loop behavior by selecting actions $a_t=K_t s_t$.
Plot the state trajectory for a handful of initial conditions.

On a single plot, show the parameters $K_t$ over time for different horizons $T$ and for the constant steady state policy. What do you observe?

Inlcude all plots in your PR.
