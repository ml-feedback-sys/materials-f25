# Paper Presentation
*Details for CS 6784 Fall 2025*

Throughout the semester, students will present selected papers (chosen from the list below) and lead a discussion. 

Presentations should start with an overview in the style of our lectures:
1. "What we do" - a description of main algorithm or method described by the paper, along with an example
2. "Why we do it" - a summary of the key results in the paper (theoretical, empirical, etc) that support the method

Presentations should contain a discussion of
 - Motivation: What are the broader motivations for this work?
 - Problem statement: What is the specific problem that the paper tries to solve?
 - Prior work: How does this paper fit into the research landscape?
 - Main ideas/results: What are the main technical results and broader takeaways?
 - Technical tools: How do the authors support the results (e.g. proof techniques or experimental setup)? Be prepared to explain this in detail.
 - Future work: What are some shortcomings of this paper or directions for future work?

Students should elicit questions and comments from the class at multiple points during the presentation. The presentation should also highlight points of confusion or disagreement to discuss as a class. 

Students are required to schedule a meeting with the TA to go over their presentation at least two days before they are scheduled to present. 

## List of papers

Prediction: Principled approaches for nonlinear sequences

1. [\[2302.14753\] Learning Hidden Markov Models Using Conditional Samples](https://arxiv.org/abs/2302.14753) 
2. [\[2508.11990\] Universal Learning of Nonlinear Dynamics](https://arxiv.org/abs/2508.11990) 
3. [\[2107.10878\] Bagging, optimized dynamic mode decomposition (BOP-DMD) for robust, stable forecasting with spatial and temporal uncertainty-quantification](https://arxiv.org/abs/2107.10878)
4. [\[2502.12465\] Computational-Statistical Tradeoffs at the Next-Token Prediction Barrier: Autoregressive and Imitation Learning under Misspecification](https://arxiv.org/abs/2502.12465) 

Prediction: In-context learning for sequences in LLMs

5. [\[2111.02080\] An Explanation of In-context Learning as Implicit Bayesian Inference](https://arxiv.org/abs/2111.02080) 
6. [\[2306.00297\] Transformers learn to implement preconditioned gradient descent for in-context learning](https://arxiv.org/abs/2306.00297) 
7. [\[2402.00795\] LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law](https://arxiv.org/abs/2402.00795)
8. [\[2402.11004\] The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains](https://arxiv.org/abs/2402.11004) 
9. [\[2508.07208\] What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains](https://arxiv.org/abs/2508.07208)

Prediction: Importance of architecture

10. [\[2402.01032\] Repeat After Me: Transformers are Better than State Space Models at Copying](https://arxiv.org/abs/2402.01032) 
11. [\[2402.18668\] Simple linear attention language models balance the recall-throughput tradeoff](https://arxiv.org/abs/2402.18668) 
12. [\[2410.01201\] Were RNNs All We Needed?](https://arxiv.org/abs/2410.01201)

Action: Principled approaches

13. [\[2402.03110\] Non-Stationary Latent Auto-Regressive Bandits](https://arxiv.org/abs/2402.03110) 
14. [\[2204.02322\] On Global and Local Convergence of Iterative Linear Quadratic Optimization Algorithms for Discrete Time Nonlinear Control](https://arxiv.org/abs/2204.02322) 
15. [\[2401.14534\] Meta-Learning Linear Quadratic Regulators: A Policy Gradient MAML Approach for Model-free LQR](https://arxiv.org/abs/2401.14534) 
16. [\[2210.09206\] Model Predictive Control via On-Policy Imitation Learning](https://arxiv.org/abs/2210.09206) 
17. [\[2503.09722\] The Pitfalls of Imitation Learning when Actions are Continuous](https://arxiv.org/abs/2503.09722)

Action: Modern techniques

18. [\[2106.01345\] Decision Transformer: Reinforcement Learning via Sequence Modeling](https://arxiv.org/abs/2106.01345) 
19. [\[2402.11450\] Learning to Learn Faster from Human Feedback with Language Model Predictive Control](https://arxiv.org/abs/2402.11450) 
20. [\[2410.23916\] Transformer-based Model Predictive Control: Trajectory Optimization via Sequence Modeling](https://arxiv.org/abs/2410.23916) 
21. [\[2503.15095\] Diffusion-Based Forecasting for Uncertainty-Aware Model Predictive Control](https://arxiv.org/abs/2503.15095) 
22. [\[2410.05364\] Diffusion Model Predictive Control](https://arxiv.org/abs/2410.05364)
